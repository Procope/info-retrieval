{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##\n",
    "\n",
    "### Florian Mohnert, Mario Giulianelli, Akash Raj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import Image\n",
    "from math import log,exp\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import pyndri\n",
    "import pyndri.compat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "# print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "# print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "# print(\"Query by tokens:\", query_tokens)\n",
    "# query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "# print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "# query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "# print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "# print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "# print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "# with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "#     print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "term2numdocs = collections.defaultdict(int)\n",
    "total_terms = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "    \n",
    "    for word in list(document_bow.keys()):\n",
    "        \n",
    "        term2numdocs[word]+=1\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IDF(query_term_id):\n",
    "    \"\"\"\n",
    "    Inverse Document Frequency of a query term.\n",
    "    \n",
    "    :param query_token_id: the query term id \n",
    "    \"\"\"\n",
    "    df = len(list(inverted_index[query_term_id]))\n",
    "                       \n",
    "    return log(num_documents / df)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    TF-IDF scoring function for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"\n",
    "    return document_term_freq * IDF(query_term_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BM25(int_document_id, query_term_id, document_term_freq, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 scoring function for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    :param k1: define how fast belief saturates\n",
    "    :param b: control the level of normalization\n",
    "    \"\"\"\n",
    "    tf = document_term_freq\n",
    "    idf = IDF(query_term_id)\n",
    "    doc_len = document_lengths[int_doc_id]\n",
    "\n",
    "    return (k1 + 1) * tf / (k1 * ((1 - b) + b * (doc_len / avg_doc_length)) + tf) * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jelinek_mercer(int_document_id, query_term_id, document_term_freq, **kwargs):\n",
    "    \"\"\"\n",
    "    Retrieval language model with Jelinek-Mercer smoothing.\n",
    "    Returns the LM score for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term\n",
    "    :param l: the interpolation parameter\n",
    "    \"\"\"\n",
    "    l = kwargs[\"lambda\"]\n",
    "    \n",
    "    d = document_lengths[int_doc_id]\n",
    "    document_score = document_term_freq / d\n",
    "    collection_score =  collection_frequencies[query_term_id] / total_terms\n",
    "        \n",
    "    return l * document_score + (1 - l) * collection_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dirichlet_prior(int_document_id, query_term_id, document_term_freq, **kwargs): \n",
    "    \"\"\"\n",
    "    Retrieval language model with Dirichlet prior smoothing.\n",
    "    Returns the LM score for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    :mu: hyperparameter\n",
    "    \"\"\"\n",
    "    mu = kwargs[\"mu\"]\n",
    "    \n",
    "    collection_freq_w = collection_frequencies[query_term_id]\n",
    "    collection_prior = collection_freq_w / total_terms\n",
    "    d = document_lengths[int_document_id]\n",
    "    \n",
    "    score = (document_term_freq + mu * collection_prior) / (d + mu)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def absolute_discounting(int_document_id, query_term_id, document_term_freq, **kwargs):\n",
    "    \"\"\"\n",
    "    Retrieval language model with Absolute Discounting smoothing.\n",
    "    Returns the LM score for a document and a query term.\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    :param delta: hyperparameter\n",
    "    \"\"\"\n",
    "    delta = kwargs[\"delta\"]\n",
    "    \n",
    "    collection_freq_w = collection_frequencies[query_term_id]\n",
    "    collection_prior = collection_freq_w / total_terms\n",
    "    \n",
    "    d = document_lengths[int_document_id]\n",
    "    unique_words = unique_terms_per_document[int_document_id]\n",
    "    \n",
    "    score = (max(document_term_freq - delta, 0) / d) + (delta * unique_words / d) * collection_prior\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kernel function definitions\n",
    "\n",
    "def passage_kernel(i, j, sigma=50):\n",
    "    '''\n",
    "    Non-proximity based Passage kernel (baseline)\n",
    "    '''\n",
    "    proximity = abs(i - j)\n",
    "\n",
    "    return 1 if proximity <= sigma else 0\n",
    "\n",
    "def gaussian_kernel(i, j, sigma=50):\n",
    "    proximity = abs(i - j)\n",
    "\n",
    "    return np.exp(-proximity ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "def triangle_kernel(i, j, sigma=50):\n",
    "    proximity = abs(i - j)\n",
    "\n",
    "    if proximity > sigma:\n",
    "        return 0\n",
    "\n",
    "    return (1 - proximity / sigma)\n",
    "\n",
    "def hamming_kernel(i, j, sigma=50):\n",
    "    '''\n",
    "    Cosine kernel\n",
    "    '''\n",
    "    proximity = abs(i - j)\n",
    "    \n",
    "    if proximity > sigma:\n",
    "        return 0\n",
    "    \n",
    "    return 0.5 * (1 + np.cos(proximity * np.pi / sigma))\n",
    "\n",
    "def circle_kernel(i, j, sigma=50):\n",
    "    proximity = abs(i - j)\n",
    "    \n",
    "    if proximity > sigma:\n",
    "        return 0\n",
    "    \n",
    "    return np.sqrt(1 - (proximity / sigma) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Tests\n",
    "assert passage_kernel(0, 0), 0\n",
    "assert gaussian_kernel(0, 0), 1\n",
    "assert triangle_kernel(0 ,0), 0\n",
    "assert hamming_kernel(0, 0), 0\n",
    "assert circle_kernel(0, 0), 0\n",
    "\n",
    "pos = 5\n",
    "doc_len = 100\n",
    "score = 0\n",
    "\n",
    "for i in range(1, doc_len + 1):\n",
    "    score += gaussian_kernel(pos, i)\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 5 minutes to run. Read from the binary file general_lm\n",
    "\n",
    "# def general_collection_lm():\n",
    "#     '''\n",
    "#     General collection lm, p(w | C)\n",
    "#     '''\n",
    "#     glm = defaultdict(lambda:0)\n",
    "\n",
    "#     for doc_id in np.arange(index.document_base(), index.maximum_document()):\n",
    "#         document = index.document(doc_id)\n",
    "#         for word in document[1]:\n",
    "#             glm[word] += 1\n",
    "\n",
    "#     for k in glm.keys():\n",
    "#         glm[k] /= sum(glm.values())\n",
    "\n",
    "#     return glm\n",
    "\n",
    "# general_lm = general_collection_lm()\n",
    "\n",
    "def collection_freq_all():\n",
    "    '''\n",
    "    General collection lm, p(w | C)\n",
    "    '''\n",
    "    glm = defaultdict(lambda:0)\n",
    "\n",
    "    for doc_id in np.arange(index.document_base(), index.maximum_document()):\n",
    "        document = index.document(doc_id)\n",
    "        for word in document[1]:\n",
    "            glm[word] += 1\n",
    "\n",
    "   \n",
    "\n",
    "    return glm\n",
    "\n",
    "#general_lm = general_collection_lm()\n",
    "collection_freq_all = collection_freq_all()\n",
    "\n",
    "\n",
    "with open('collection_freq_all', 'wb+') as f:\n",
    "    pickle.dump(dict(collection_freq_all), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('general_lm', 'wb+') as f:\n",
    "    #pickle.dump(dict(general_lm), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('general_lm', 'rb') as f:\n",
    "    #general_lm = pickle.load(f)\n",
    "\n",
    "with open('collection_freq_all', 'rb') as f:\n",
    "    collection_freq_all = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top 1000 documents (TF-IDF) - tfidf_data\n",
    "\n",
    "def doc_word_pos_util(Q, D):\n",
    "    '''\n",
    "    Returns a dictionary with the position of each occurence of word 'w' in Document 'D'\n",
    "    '''\n",
    "    resp = {}\n",
    "\n",
    "    for i, w in enumerate(D):\n",
    "        if w not in resp.keys():\n",
    "            resp[w] = [i]\n",
    "        else:\n",
    "            resp[w].append(i)\n",
    "\n",
    "    return resp\n",
    "\n",
    "\n",
    "def plm_model_smoothed(i, w, D, doc_length, mu=500):\n",
    "    '''\n",
    "    p(w|D, i))\n",
    "    \n",
    "    Args:\n",
    "        i: position\n",
    "        w: word\n",
    "        D: dictionary containing the positions where w occurs\n",
    "        mu: for smoothing\n",
    "        \n",
    "    Return:\n",
    "        np array of length 5 containing the scores for each kernel\n",
    "    '''\n",
    "    plm = np.zeros(5)\n",
    "\n",
    "    if w in D:\n",
    "        for j in D[w]:\n",
    "            plm[0] += passage_kernel(i, j)\n",
    "            plm[1] += gaussian_kernel(i, j)\n",
    "            plm[2] += triangle_kernel(i, j)\n",
    "            plm[3] += hamming_kernel(i, j)\n",
    "            plm[4] += circle_kernel(i, j)\n",
    "\n",
    "    # general collection LM is used as the background model\n",
    "    plm += (mu * general_lm[w])\n",
    "\n",
    "    # Calculate Z_i. TODO: Use the optimiztion given in the paper\n",
    "    z_i = np.zeros(5)\n",
    "\n",
    "    plm[0] /= (sum([passage_kernel(i, j) for j in range(1, doc_length + 1)]) + mu)\n",
    "    plm[1] /= (sum([gaussian_kernel(i, j) for j in range(1, doc_length + 1)]) + mu)\n",
    "    plm[2] /= (sum([triangle_kernel(i, j) for j in range(1, doc_length + 1)]) + mu)\n",
    "    plm[3] /= (sum([hamming_kernel(i, j) for j in range(1, doc_length + 1)]) + mu)\n",
    "    plm[4] /= (sum([circle_kernel(i, j) for j in range(1, doc_length + 1)]) + mu)\n",
    "\n",
    "    return plm\n",
    "\n",
    "\n",
    "def plm_score(i, Q, D, mu=500):\n",
    "    '''\n",
    "    Score each PLM using Kullback-Leibler divergence retrieval model\n",
    "\n",
    "    Args:\n",
    "        i: position\n",
    "        Q: Query\n",
    "        D: Document\n",
    "    '''\n",
    "    # Query likelihood model - 1 / query length\n",
    "    query_lm = 1.0 / len(Q)\n",
    "\n",
    "    # Kullback-Leibler divergence retrieval model\n",
    "    kl_divergence = np.zeros(5)\n",
    "\n",
    "    resp = doc_word_pos_util(Q, D[1])\n",
    "\n",
    "    for w in Q:\n",
    "        if w in D[1]:\n",
    "            plm = plm_model_smoothed(i, w, resp, len(D[1]), mu)\n",
    "            for k in range(5):\n",
    "                kl_divergence[k] += (- query_lm * math.log(query_lm / plm[k]))\n",
    "\n",
    "    return kl_divergence\n",
    "\n",
    "\n",
    "def best_position_strategy(Q, D, mu=500):\n",
    "    '''\n",
    "    S(Q, D) = max_i {S(Q, D, i)}\n",
    "    '''\n",
    "    # Initialize KL-D for all the 5 kernels\n",
    "    max_score = [-99999999] * 5\n",
    "\n",
    "    for i in range(1, len(D) + 1):\n",
    "        scores = plm_score(i, Q, D, mu)\n",
    "\n",
    "        for k in range(5):\n",
    "            if max_score[k] < scores[k]:\n",
    "                max_score[k] = scores[k]\n",
    "\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define, for convenience, a dictionary mapping external\n",
    "# document ids to internal document document ids. \n",
    "\n",
    "ext_to_int = {}\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, _ = index.document(int_doc_id)\n",
    "    ext_to_int[ext_doc_id] = int_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read TREC ground truth file containing the VALIDATION SET.\n",
    "# Create dictionary validation_ids that maps queries to internal and external docID pairs.\n",
    "# validation_ids: queryID --> (internal_doc_id, external_doc_id)\n",
    "\n",
    "with open(\"ap_88_89/qrel_validation\", \"r\") as f:\n",
    "    validation_ids = {}\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        q_id = line[0]\n",
    "        ext_id = line[2]\n",
    "        \n",
    "        try:\n",
    "            int_id = ext_to_int[ext_id]\n",
    "            \n",
    "            try:\n",
    "                validation_ids[q_id].append((int_id, ext_id))\n",
    "            except KeyError:\n",
    "                validation_ids[q_id] = [(int_id, ext_id)]\n",
    "            \n",
    "        except KeyError:\n",
    "            pass  # validation document not in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read TREC ground truth file containing the TEST SET.\n",
    "# Create dictionary test_ids that maps queries to internal and external docID pairs.\n",
    "# test_ids: queryID --> (internal_doc_id, external_doc_id)\n",
    "\n",
    "with open(\"ap_88_89/qrel_test\",\"r\") as f:\n",
    "    test_ids = {}\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        q_id = line[0]\n",
    "        ext_id = line[2]\n",
    "        \n",
    "        try:\n",
    "            int_id = ext_to_int[ext_id]\n",
    "        \n",
    "            try:\n",
    "                test_ids[q_id].append((int_id, ext_id))\n",
    "            except KeyError:\n",
    "                test_ids[q_id] = [(int_id, ext_id)]\n",
    "                \n",
    "        except KeyError:\n",
    "            pass  # test document not in index\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, **kwargs):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"This output file ({}) already exists.\\n\".format(run_out_path),\n",
    "              \"Delete this file, rename it, or move it elsewhere in your file system.\\n\", sep='')\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        dataset = kwargs.pop('dataset')\n",
    "    except KeyError:\n",
    "        # use test set if user does not express a preference\n",
    "        dataset = 'test'\n",
    "    \n",
    "    if dataset == 'test':\n",
    "        query_to_docs = test_ids\n",
    "    elif dataset == 'validation':\n",
    "        query_to_docs = validation_ids\n",
    "    else:\n",
    "        raise ValueError(\"Two possible values for dataset: 'test' or 'validation'.\")\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    print('Retrieval model: {}'.format(model_name))\n",
    "    retrieval_start_time = time.time()\n",
    "    \n",
    "    # Fill the data dictionary. \n",
    "    # The dictionary data has the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    # FOR ALL QUERIES\n",
    "    for query_id in query_to_docs.keys():  \n",
    "        # FOR ALL DOCUMENTS\n",
    "        for int_doc_id, ext_doc_id in query_to_docs[query_id]:  \n",
    "                \n",
    "            # discard all zero length docs\n",
    "            if document_lengths[int_doc_id] == 0:\n",
    "                    continue\n",
    "                    \n",
    "            score = 0\n",
    "            cnt_qwords_in_doc = 0  # how many query words occ\n",
    " \n",
    "            # FOR ALL QUERY TERMS\n",
    "            for q_term in tokenized_queries[query_id]: \n",
    "                try: \n",
    "                    document_term_freq = inverted_index[q_term][int_doc_id]\n",
    "                    cnt_qwords_in_doc += 1\n",
    "                except KeyError:\n",
    "                    document_term_freq = 0  # query term is not in the document\n",
    "\n",
    "                if model_name == \"tfidf\" or model_name == \"BM25\":\n",
    "                    score += score_fn(int_doc_id, q_term, document_term_freq, **kwargs)\n",
    "                else:\n",
    "                    score += log(score_fn(int_doc_id, q_term, document_term_freq, **kwargs))\n",
    "\n",
    "            # only score documents that contain at least one query term\n",
    "            if cnt_qwords_in_doc == 0:\n",
    "                continue\n",
    "\n",
    "            # transform back from log-probs to probs\n",
    "            if model_name != \"tfidf\" and model_name != \"BM25\": \n",
    "                score = exp(score)\n",
    "            \n",
    "            # Fill data dictionary\n",
    "            try:\n",
    "                data[query_id].append((score, ext_doc_id))\n",
    "            except:\n",
    "                data[query_id] = [(score, ext_doc_id)]\n",
    "    \n",
    "    # Write output to TREC .run file\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)  # only consider top 1000 docs for each query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run all retrieval methods for all the VALIDATION queries and write the TREC-friendly results to a file. \n",
    "\n",
    "# run_retrieval('tfidf', tfidf, **{\"dataset\":'test'})\n",
    "# run_retrieval('BM25', BM25, **{\"dataset\":'test'})\n",
    "\n",
    "# run_retrieval('jelinek_mercer_0.1', jelinek_mercer, **{\"dataset\":'test', \"lambda\":0.1})\n",
    "# # run_retrieval('jelinek_mercer_0.5', jelinek_mercer, **{\"dataset\":'test', \"lambda\":0.5})\n",
    "# # run_retrieval('jelinek_mercer_0.9', jelinek_mercer, **{\"dataset\":'test', \"lambda\":0.9})\n",
    "\n",
    "# run_retrieval('dirichlet_prior_500', dirichlet_prior, **{\"dataset\":'test', \"mu\":500})\n",
    "# # run_retrieval('dirichlet_prior_1000', dirichlet_prior, **{\"dataset\":'test', \"mu\":1000})\n",
    "# # run_retrieval('dirichlet_prior_1500', dirichlet_prior, **{\"dataset\":'test', \"mu\":1500})\n",
    "\n",
    "# # run_retrieval('absolute_discounting_0.1', absolute_discounting, **{\"dataset\":'test', \"delta\":0.1})\n",
    "# # run_retrieval('absolute_discounting_0.5', absolute_discounting, **{\"dataset\":'test', \"delta\":0.5})\n",
    "# run_retrieval('absolute_discounting_0.9', absolute_discounting, **{\"dataset\":'test', \"delta\":0.9})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLM TEST\n",
    "\n",
    "query_id = list(validation_ids.keys())[0]\n",
    "query_terms = tokenized_queries[query_id]\n",
    "first_query_doc = validation_ids[query_id][0]\n",
    "\n",
    "print('query_id:', query_id)\n",
    "print('query_terms:', tokenized_queries[query_id])\n",
    "print('first_query_doc:', validation_ids[query_id][0])\n",
    "\n",
    "doc = index.document(first_query_doc[0])\n",
    "\n",
    "scores = best_position_strategy(query_terms, doc)\n",
    "\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PLM\n",
    "\n",
    "doc_scores = []\n",
    "\n",
    "for doc_id in validation_ids[query_id]:\n",
    "    doc = index.document(doc_id[0])\n",
    "\n",
    "    scores = best_position_strategy(query_terms, doc)\n",
    "    \n",
    "    # Store only values for gaussian\n",
    "    doc_scores.append((doc_id[0], scores[0]))\n",
    "\n",
    "sorted_by_second = list(reversed(sorted(doc_scores, key=lambda tup: tup[1])))\n",
    "\n",
    "new_array = []\n",
    "for item in sorted_by_second:\n",
    "    if item[1] < 0:\n",
    "        new_array.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plotting NDCG@10 for different hyperparamaters ####\n",
    "\n",
    "def read_statistics(filepath):\n",
    "    '''\n",
    "    Read statistics from TREC_EVAL output file and store them in an array.\n",
    "    '''\n",
    "    data = []\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "\n",
    "            data.append(float(line[2]))\n",
    "        \n",
    "    return data[:-1]\n",
    "\n",
    "\n",
    "def plot_ndcg_stats(data, labels):\n",
    "    '''\n",
    "    Print NDCG@10 statistics for multiple hyperparameter values.\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    data = [[list(range(len(x))), x] for x in data]\n",
    "    \n",
    "    for ndcg, color in zip(data, colors):\n",
    "        x, y = ndcg\n",
    "        label = labels[colors.index(color)]\n",
    "        ax.scatter(x, y, c=color, label=label, alpha=0.3, edgecolors='none')\n",
    "        ax.set_xlabel('queries')\n",
    "        ax.set_ylabel('NDCG@10')\n",
    "\n",
    "       \n",
    "    ax.legend()\n",
    "    ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jelinek_mercer_01_ndcg10 = read_statistics('statistics/jelinek_mercer_01_ndcg10.txt')\n",
    "jelinek_mercer_05_ndcg10 = read_statistics('statistics/jelinek_mercer_05_ndcg10.txt')\n",
    "jelinek_mercer_09_ndcg10 = read_statistics('statistics/jelinek_mercer_09_ndcg10.txt')   \n",
    "jelinek_mercer_all_ndcg10 = [jelinek_mercer_01_ndcg10, jelinek_mercer_05_ndcg10, jelinek_mercer_09_ndcg10]\n",
    "\n",
    "dirichlet_500_ndcg10 = read_statistics('statistics/dirichlet_prior_500_ndcg10.txt')\n",
    "dirichlet_1000_ndcg10 = read_statistics('statistics/dirichlet_prior_1000_ndcg10.txt')\n",
    "dirichlet_1500_ndcg10 = read_statistics('statistics/dirichlet_prior_1500_ndcg10.txt')   \n",
    "dirichlet_all_ndcg10 = [dirichlet_500_ndcg10, dirichlet_1000_ndcg10, dirichlet_1500_ndcg10]  \n",
    "\n",
    "abs_discount_01_ndcg10 = read_statistics('statistics/absolute_discounting_01_ndcg10.txt')\n",
    "abs_discount_05_ndcg10 = read_statistics('statistics/absolute_discounting_05_ndcg10.txt')\n",
    "abs_discount_09_ndcg10 = read_statistics('statistics/absolute_discounting_09_ndcg10.txt')   \n",
    "abs_discount_all_ndcg10 = [abs_discount_01_ndcg10, abs_discount_05_ndcg10, abs_discount_09_ndcg10]\n",
    "\n",
    "tfidf_ndcg10 = read_statistics('statistics/tfidf_ndcg10.txt')\n",
    "BM25_ndcg10 = read_statistics('statistics/BM25_ndcg10.txt')\n",
    "pbm_500_ndcg10 = read_statistics(\"statistics/pbm_500_ndcg10.txt\")\n",
    "    \n",
    "plot_ndcg_stats(jelinek_mercer_all_ndcg10, labels=['jelinek-mercer_01', 'jelinek-mercer_05', 'jelinek-mercer_09'])\n",
    "plot_ndcg_stats(dirichlet_all_ndcg10, labels=['dirichlet_500', 'dirichlet_1000', 'abs_discount_1500'])\n",
    "plot_ndcg_stats(abs_discount_all_ndcg10, labels=['abs_discount_01', 'abs_discount_05', 'abs_discount_09'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ndcg_stats(data, labels):\n",
    "    '''\n",
    "    Print NDCG@10 statistics for multiple hyperparameter values.\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = ['red', 'green', 'blue',\"brown\",\"yellow\",\"cyan\"]\n",
    "    data = [[list(range(len(x))), x] for x in data]\n",
    "    scale = [130,110,90,70,50]\n",
    "    for ndcg, color in zip(data, colors):\n",
    "        x, y = ndcg\n",
    "        label = labels[colors.index(color)]\n",
    "        ax.scatter(x, y, c=color, label=label, alpha=0.4, edgecolors='none', s = scale[colors.index(color)])\n",
    "        minor_ticks = np.arange(0,31,1)\n",
    "        ax.set_xticks(minor_ticks)\n",
    "        ax.set_xlabel('queries')\n",
    "        ax.set_ylabel('NDCG@10')\n",
    "\n",
    "       \n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "\n",
    "    ax.grid(axis = \"x\", which='minor', linestyle=\":\")\n",
    "\n",
    "fig_size = [14,10]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "best_comparison = [tfidf_ndcg10, BM25_ndcg10, abs_discount_09_ndcg10,dirichlet_500_ndcg10,jelinek_mercer_01_ndcg10,pbm_500_ndcg10]\n",
    "\n",
    "\n",
    "plot_ndcg_stats(best_comparison, labels=[\"tfidf\", \"BM25\",  'abs_discounting_09', 'dirichlet_500', 'jelinek-mercer_01',\"pbm_500\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_map1000_test = read_statistics('statistics/test/tfidf_map1000_test.txt')\n",
    "bm25_map1000_test = read_statistics('statistics/test/bm25_map1000_test.txt')\n",
    "jelinek_mercer_map1000_test = read_statistics('statistics/test/jelinek_mercer_01_map1000_test.txt')\n",
    "abs_disc_map1000_test = read_statistics('statistics/test/absolute_discounting_09_map1000_test.txt')\n",
    "dirichlet_map1000_test = read_statistics('statistics/test/dirichlet_prior_500_map1000_test.txt')\n",
    "\n",
    "tfidf_ndcg10_test = read_statistics('statistics/test/tfidf_ndcg10_test.txt')\n",
    "bm25_ndcg10_test = read_statistics('statistics/test/bm25_ndcg10_test.txt')\n",
    "jelinek_mercer_ndcg10_test = read_statistics('statistics/test/jelinek_mercer_01_ndcg10_test.txt')\n",
    "abs_disc_ndcg10_test = read_statistics('statistics/test/absolute_discounting_09_ndcg10_test.txt')\n",
    "dirichlet_ndcg10_test = read_statistics('statistics/test/dirichlet_prior_500_ndcg10_test.txt')\n",
    "\n",
    "tfidf_precision5_test = read_statistics('statistics/test/tfidf_precision5_test.txt')\n",
    "bm25_precision5_test = read_statistics('statistics/test/bm25_precision5_test.txt')\n",
    "jelinek_mercer_precision5_test = read_statistics('statistics/test/jelinek_mercer_01_precision5_test.txt')\n",
    "abs_disc_precision5_test = read_statistics('statistics/test/absolute_discounting_09_precision5_test.txt')\n",
    "dirichlet_precision5_test = read_statistics('statistics/test/dirichlet_prior_500_precision5_test.txt')\n",
    "\n",
    "tfidf_recall1000_test = read_statistics('statistics/test/tfidf_recall1000_test.txt')\n",
    "bm25_recall1000_test = read_statistics('statistics/test/bm25_recall1000_test.txt')\n",
    "jelinek_mercer_recall1000_test = read_statistics('statistics/test/jelinek_mercer_01_recall1000_test.txt')\n",
    "abs_disc_recall1000_test = read_statistics('statistics/test/absolute_discounting_09_recall1000_test.txt')\n",
    "dirichlet_recall1000_test = read_statistics('statistics/test/dirichlet_prior_500_recall1000_test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map1000_test_all = {'tfidf_map1000_test': tfidf_map1000_test, \n",
    "                    'bm25_map1000_test': bm25_map1000_test, \n",
    "                    'jelinek_mercer_map1000_test': jelinek_mercer_map1000_test,\n",
    "                    'abs_disc_map1000_test': abs_disc_map1000_test,\n",
    "                    'dirichlet_map1000_test': dirichlet_map1000_test}\n",
    "\n",
    "ndcg10_test_all = {'tfidf_ndcg10_test': tfidf_ndcg10_test, \n",
    "                    'bm25_ndcg10_test': bm25_ndcg10_test, \n",
    "                    'jelinek_mercer_ndcg10_test': jelinek_mercer_ndcg10_test,\n",
    "                    'abs_disc_ndcg10_test': abs_disc_ndcg10_test,\n",
    "                    'dirichlet_ndcg10_test': dirichlet_ndcg10_test}\n",
    "\n",
    "precision5_test_all = {'tfidf_precision5_test': tfidf_precision5_test, \n",
    "                    'bm25_precision5_test': bm25_precision5_test, \n",
    "                    'jelinek_mercer_precision5_test': jelinek_mercer_precision5_test,\n",
    "                    'abs_disc_precision5_test': abs_disc_precision5_test,\n",
    "                    'dirichlet_precision5_test': dirichlet_precision5_test}\n",
    "\n",
    "recall1000_test_all = {'tfidf_recall1000_test': tfidf_recall1000_test, \n",
    "                    'bm25_recall1000_test': bm25_recall1000_test, \n",
    "                    'jelinek_mercer_recall1000_test': jelinek_mercer_recall1000_test,\n",
    "                    'abs_disc_recall1000_test': abs_disc_recall1000_test,\n",
    "                    'dirichlet_recall1000_test': dirichlet_recall1000_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def t_test(metric_comparison_dict, p=0.0125):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    comparisons = list(itertools.combinations(metric_comparison_dict.keys(), 2))\n",
    "    \n",
    "    for metric1, metric2 in comparisons:\n",
    "        scores1 = metric_comparison_dict[metric1]\n",
    "        scores2 = metric_comparison_dict[metric2]\n",
    "        p_value = ttest_rel(scores1, scores2)[1]\n",
    "        \n",
    "        if p_value < p:\n",
    "            print(\"{} vs. {}\\nSignificant p-value (p < {}): {}\\n\".format(metric1, metric2, p, p_value))\n",
    "        else:\n",
    "            print(\"{} vs. {}\\nNon-significant p-value (p >= {}): {}\\n\".format(metric1, metric2, p, p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t_test(map1000_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t_test(ndcg10_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t_test(recall1000_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t_test(precision5_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "from scipy.stats import entropy as kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_filter(n=1000, dataset='validation'):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    if dataset == 'test':\n",
    "        query_to_docs = test_ids\n",
    "    elif dataset == 'validation':\n",
    "        query_to_docs = validation_ids\n",
    "    else:\n",
    "        raise ValueError(\"Two possible values for dataset: 'test' or 'validation'.\")\n",
    "    \n",
    "    \n",
    "    query_to_top_n = {}\n",
    "    \n",
    "    # FOR ALL QUERIES\n",
    "    for query_id in query_to_docs.keys():  \n",
    "        for int_doc_id, _ in query_to_docs[query_id]:  \n",
    "                \n",
    "            # discard all zero length docs\n",
    "            if document_lengths[int_doc_id] == 0:\n",
    "                continue\n",
    "                    \n",
    "            score = 0\n",
    "            cnt_qwords_in_doc = 0  # how many query words occ\n",
    " \n",
    "            # FOR ALL QUERY TERMS\n",
    "            for q_term in tokenized_queries[query_id]: \n",
    "                try: \n",
    "                    document_term_freq = inverted_index[q_term][int_doc_id]\n",
    "                    score += tfidf(int_doc_id, q_term, document_term_freq)\n",
    "                    cnt_qwords_in_doc += 1\n",
    "\n",
    "                # query term is not in the document\n",
    "                except KeyError:\n",
    "                    document_term_freq = 0\n",
    "                    score += tfidf(int_doc_id, q_term, document_term_freq)\n",
    "\n",
    "            # only score documents that contain at least one query term\n",
    "            if cnt_qwords_in_doc == 0:\n",
    "                continue\n",
    "\n",
    "            # Fill data dictionary\n",
    "            try:\n",
    "                data[query_id].append((score, int_doc_id))\n",
    "            except:\n",
    "                data[query_id] = [(score, int_doc_id)]\n",
    "                \n",
    "        scores = [x[0] for x in data[query_id]]\n",
    "        int_doc_ids = [x[1] for x in data[query_id]]\n",
    "        \n",
    "        scores, int_doc_ids = zip(*sorted(zip(scores, int_doc_ids), key=operator.itemgetter(0), reverse=True))\n",
    "        query_to_top_n[query_id] = int_doc_ids[:n]\n",
    "        \n",
    "    return query_to_top_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lsm_retrieval(model_name, model, query_to_top_n, similarity_index=None, dataset='validation'):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example)\n",
    "    \"\"\"\n",
    "    model_name = model_name.upper()\n",
    "    \n",
    "    if model_name[:3] == 'LSI':\n",
    "        use_lsi = True\n",
    "        \n",
    "        if not similarity_index:\n",
    "            raise ValueError(\"Similarity index must be provided for LSI retrieval.\")\n",
    "            \n",
    "    elif model_name[:3] == 'LDA':\n",
    "        use_lsi = False\n",
    "    else:\n",
    "        raise ValueError(\"Two available models: 'LSI' or 'LDA'.\")\n",
    "        \n",
    "    run_out_path = '{}.run'.format(model_name.lower())\n",
    "    \n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"This output file ({}) already exists.\\n\".format(run_out_path),\n",
    "              \"Delete this file, rename it, or move it elsewhere in your file system.\\n\", sep='')\n",
    "        return\n",
    "    \n",
    "    if dataset == 'test':\n",
    "        query_to_docs = test_ids\n",
    "    elif dataset == 'validation':\n",
    "        query_to_docs = validation_ids\n",
    "    else:\n",
    "        raise ValueError(\"Two possible values for dataset: 'test' or 'validation'.\")\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    print('Retrieval model: {}'.format(model_name))\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    \n",
    "    # Fill the data dictionary. \n",
    "    # The dictionary data has the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    # FOR ALL QUERIES\n",
    "    for query_id in query_to_docs.keys(): \n",
    "        query_counter = collections.Counter(token_id \n",
    "                                            for token_id in token_ids\n",
    "                                            if token_id > 0\n",
    "                                           )\n",
    "        query_terms = list(query_counter.items())\n",
    "        query_tfidf = tfidf_model[query_terms]\n",
    "        query_vec = model[query_tfidf]\n",
    "        \n",
    "        if use_lsi:\n",
    "            similarities = similarity_index[query_vec]\n",
    "        else:\n",
    "            query_distribution = [x[1] for x in query_vec]\n",
    "        \n",
    "        for int_doc_id in query_to_top_n[query_id]:\n",
    "            if use_lsi:\n",
    "                score = similarities[int_doc_id - 1]\n",
    "            else:\n",
    "                doc_tfidf = tfidf_corpus[int_doc_id - 1]\n",
    "                doc_distribution = [x[1] for x in model[doc_tfidf]]\n",
    "                score = - kl_divergence(query_distribution, doc_distribution)\n",
    "            \n",
    "            ext_doc_id, _ = index.document(int_doc_id)\n",
    "            \n",
    "            # Fill data dictionary\n",
    "            try:\n",
    "                data[query_id].append((score, ext_doc_id))\n",
    "            except:\n",
    "                data[query_id] = [(score, ext_doc_id)]\n",
    "    \n",
    "    # Write output to TREC .run file\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)  # only consider top 1000 docs for each query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_to_top1000_validation = tfidf_filter(n=1000, dataset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_to_top1000_test = tfidf_filter(n=1000, dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, id2token, _ = index.get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_to_bow():\n",
    "    corpus = []\n",
    "\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        _, token_ids = index.document(int_doc_id)\n",
    "\n",
    "        text_bow = collections.Counter(token_id \n",
    "                                       for token_id in token_ids\n",
    "                                       if token_id > 0)\n",
    "        corpus.append(text_bow)\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus_bow = [list(counter.items()) for counter in corpus_to_bow()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_model = TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_corpus = [x for x in tfidf_model[corpus_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi_models_numtopics = {}\n",
    "# lsi_similarity_indices_numtopics = {}\n",
    "\n",
    "# for num_topics in [3, 10, 15, 20, 40, 70, 100, 200, 300, 400]:\n",
    "#     iter_start_time = time.time()\n",
    "\n",
    "#     lsi = LsiModel(tfidf_corpus,\n",
    "#                    id2word=id2token,\n",
    "#                    num_topics=num_topics,\n",
    "#                    chunksize=20000,\n",
    "#                    onepass=True,\n",
    "#                    power_iters=2)\n",
    "\n",
    "#     lsi_models_numtopics[num_topics] = lsi\n",
    "#     lsi_similarity_indices_numtopics[num_topics] = similarities.MatrixSimilarity(\n",
    "#                                                                 lsi[tfidf_corpus],\n",
    "#                                                                 num_features=num_topics\n",
    "#                                                             )\n",
    "#     print('Number of topics: {}.  {} seconds'.format(num_topics, int(time.time() - iter_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for param_setting, lsi_model in lsi_models_numtopics.items():\n",
    "#     run_lsm_retrieval(model_name='lsi_numtopics_{}'.format(param_setting),\n",
    "#                           model=lsi_model,\n",
    "#                           similarity_index=lsi_similarity_indices_numtopics[param_setting],\n",
    "#                           query_to_top_n=query_to_top1000,\n",
    "#                           dataset='validation')\n",
    "    \n",
    "# # # LSI_NUMTOPICS_40 is the best model: ndcg_cut_10: 0.2263\n",
    "\n",
    "# lsi_models_numtopics = None\n",
    "# lsi_similarity_indices_numtopics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lsi_models_chunksize = {}\n",
    "# lsi_similarity_indices_chunksize = {}\n",
    "\n",
    "# for chunksize in [5000, 10000, 20000, 30000]:\n",
    "#     print('Chunksize: {}'.format(chunksize))\n",
    "#     iter_start_time = time.time()\n",
    "\n",
    "#     lsi = LsiModel(tfidf_corpus,\n",
    "#                    id2word=id2token,\n",
    "#                    num_topics=200,\n",
    "#                    chunksize=chunksize,\n",
    "#                    onepass=True,\n",
    "#                    power_iters=2)\n",
    "\n",
    "#     lsi_models_chunksize[chunksize] = lsi\n",
    "#     lsi_similarity_indices_chunksize[chunksize] = similarities.MatrixSimilarity(\n",
    "#                                                                 lsi[tfidf_corpus],\n",
    "#                                                                 num_features=200\n",
    "#                                                             )\n",
    "#     print('Runtime: {} seconds'.format(int(time.time() - iter_start_time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for param_setting, lsi_model in lsi_models_chunksize.items():\n",
    "#     run_lsm_retrieval(model_name='lsi_chunksize_{}'.format(param_setting),\n",
    "#                           model=lsi_model,\n",
    "#                           similarity_index=lsi_similarity_indices_chunksize[param_setting],\n",
    "#                           query_to_top_n=query_to_top1000,\n",
    "#                           dataset='validation')\n",
    "    \n",
    "# # LSI_CHUNKSIZE_5000 is the best model: ndcg_cut_10 = 0.2190\n",
    "# # but is it worth it? 20000 is second best with ndcg_cut_10 = 0.2173\n",
    "\n",
    "# lsi_models_chunksize = None\n",
    "# lsi_similarity_indices_chunksize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lsi_models_onepass = {}\n",
    "# lsi_similarity_indices_onepass = {}\n",
    "\n",
    "# for onepass in [False, True]:\n",
    "#     if onepass:\n",
    "#         print('One-pass')\n",
    "#     else:\n",
    "#         print('Multi-pass')\n",
    "#     iter_start_time = time.time()\n",
    "\n",
    "#     lsi = LsiModel(tfidf_corpus,\n",
    "#                    id2word=id2token,\n",
    "#                    num_topics=200,\n",
    "#                    chunksize=20000,\n",
    "#                    onepass=onepass,\n",
    "#                    power_iters=2)\n",
    "\n",
    "#     lsi_models_onepass[onepass] = lsi\n",
    "#     lsi_similarity_indices_onepass[onepass] = similarities.MatrixSimilarity(\n",
    "#                                                                 lsi[tfidf_corpus],\n",
    "#                                                                 num_features=200\n",
    "#                                                             )\n",
    "#     print('Runtime: {} seconds'.format(int(time.time() - iter_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for param_setting, lsi_model in lsi_models_onepass.items():\n",
    "#     run_lsm_retrieval(model_name='lsi_onepass_{}'.format(param_setting),\n",
    "#                           model=lsi_model,\n",
    "#                           similarity_index=lsi_similarity_indices_onepass[param_setting],\n",
    "#                           query_to_top_n=query_to_top1000,\n",
    "#                           dataset='validation')\n",
    "    \n",
    "# # LSI_ONEPASS_TRUE is the best model: ndcg_cut_10: 0.2160\n",
    "\n",
    "# lsi_models_onepass = None\n",
    "# lsi_similarity_indices_onepass = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_models_numtopics = {}\n",
    "\n",
    "for num_topics in [40, 70, 100, 130, 160]:\n",
    "    print('Number of topics: {}'.format(num_topics))\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    lda = LdaModel(tfidf_corpus,\n",
    "                   num_topics=num_topics,\n",
    "                   chunksize=2000,\n",
    "                   alpha='auto',\n",
    "                   eta='auto',\n",
    "                   minimum_probability=0.0,\n",
    "                   per_word_topics=False)\n",
    "\n",
    "    lda_models_numtopics[num_topics] = lda\n",
    "\n",
    "    print('Runtime: {} seconds'.format(int(time.time() - iter_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param_setting, lda_model in lda_models_numtopics.items():\n",
    "    run_lsm_retrieval(model_name='lda_numtopics_{}'.format(param_setting),\n",
    "                          model=lda_model,\n",
    "                          query_to_top_n=query_to_top1000,\n",
    "                          dataset='validation')\n",
    "    \n",
    "# num_topics=100 is the best model: ndcg_cut_10: 0.2909\n",
    "\n",
    "lda_models_numtopics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_models_chunksize = {}\n",
    "\n",
    "for chunksize in [500, 2000, 10000]:\n",
    "    print('Chunksize: {}'.format(chunksize))\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    lda = LdaModel(tfidf_corpus,\n",
    "                   num_topics=100,\n",
    "                   chunksize=chunksize,\n",
    "                   alpha='auto',\n",
    "                   eta='auto',\n",
    "                   minimum_probability=0.0,\n",
    "                   per_word_topics=False)\n",
    "\n",
    "    lda_models_chunksize[chunksize] = lda\n",
    "\n",
    "    print('Runtime: {} seconds'.format(int(time.time() - iter_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param_setting, lda_model in lda_models_chunksize.items():\n",
    "    run_lsm_retrieval(model_name='lda_chunksize_{}'.format(param_setting),\n",
    "                          model=lda_model,\n",
    "                          query_to_top_n=query_to_top1000,\n",
    "                          dataset='validation')\n",
    "    \n",
    "# chunksize=2000 is the best model: ndcg_cut_10: 0.2909\n",
    "\n",
    "lda_models_chunksize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = LsiModel(tfidf_corpus,\n",
    "               id2word=id2token,\n",
    "               num_topics=20,\n",
    "               chunksize=5000,\n",
    "               onepass=True,\n",
    "               power_iters=2)\n",
    "\n",
    "lsi_similarity_index = similarities.MatrixSimilarity(lsi[tfidf_corpus], num_features=20)\n",
    "\n",
    "run_lsm_retrieval(model_name='LSI',\n",
    "                  model=lsi,\n",
    "                  similarity_index=lsi_similarity_index,\n",
    "                  query_to_top_n=query_to_top1000_test,\n",
    "                  dataset='test')\n",
    "\n",
    "lsi.save('lsi_model')\n",
    "lsi_similarity_index.save('lsi_similarity_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(tfidf_corpus,\n",
    "               num_topics=100,\n",
    "               chunksize=2000,\n",
    "               alpha='auto',\n",
    "               eta='auto',\n",
    "               minimum_probability=0.0,\n",
    "               per_word_topics=False)\n",
    "\n",
    "run_lsm_retrieval(model_name='LDA',\n",
    "                  model=lda,\n",
    "                  query_to_top_n=query_to_top1000_test,\n",
    "                  dataset='test')\n",
    "\n",
    "lda = LdaModel.load('./lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_ndcg10_test = read_statistics('statistics/test/lsi_ndcg10_test.txt')\n",
    "lda_ndcg10_test = read_statistics('statistics/test/lda_ndcg10_test.txt')\n",
    "ndcg10_test_all = {'lsi_ndcg10_test': lsi_ndcg10_test, 'lda_ndcg10_test': lda_ndcg10_test}\n",
    "\n",
    "lsi_map1000_test = read_statistics('statistics/test/lsi_map1000_test.txt')\n",
    "lda_map1000_test = read_statistics('statistics/test/lda_map1000_test.txt')\n",
    "map1000_test_all = {'lsi_map1000_test': lsi_map1000_test, 'lda_map1000_test': lda_map1000_test}\n",
    "\n",
    "lsi_recall1000_test = read_statistics('statistics/test/lsi_recall1000_test.txt')\n",
    "lda_recall1000_test = read_statistics('statistics/test/lda_recall1000_test.txt')\n",
    "recall1000_test_all = {'lsi_recall1000_test': lsi_recall1000_test, 'lda_recall1000_test': lda_recall1000_test}\n",
    "\n",
    "lsi_precision5_test = read_statistics('statistics/test/lsi_precision5_test.txt')\n",
    "lda_precision5_test = read_statistics('statistics/test/lda_precision5_test.txt')\n",
    "precision5_test_all = {'lsi_precision5_test': lsi_precision5_test, 'lda_precision5_test': lda_precision5_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_test(ndcg10_test_all)\n",
    "t_test(map1000_test_all)\n",
    "t_test(recall1000_test_all)\n",
    "t_test(precision5_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting w2v for all terms\n",
    "\n",
    "\n",
    "def train_w2v():\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "    sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "    word2vec_init = gensim.models.Word2Vec(\n",
    "        size= 300,   # Embedding size\n",
    "        window=5,    # One-sided window size\n",
    "        sg=True,     # Skip-gram.\n",
    "        min_count=5,  # Minimum word frequency.\n",
    "        sample=1e-3,  # Sub-sample threshold.\n",
    "        hs=False,  # Hierarchical softmax.\n",
    "        negative=10,  # Number of negative examples.\n",
    "        iter=1,  # Number of iterations.\n",
    "        workers=8,  # Number of workers.\n",
    "    \n",
    "    )\n",
    "\n",
    "    word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "    model = word2vec_init\n",
    "\n",
    "\n",
    "\n",
    "    print(\"start training\")\n",
    "\n",
    "    model.train(sentences, total_examples = len(sentences) , epochs= 5 )\n",
    "\n",
    "\n",
    "\n",
    "    model.wv.save_word2vec_format('model_w2v.bin')\n",
    "    model.wv.save_word2vec_format('model_w2v.txt', binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IDF_all_terms(term_id):\n",
    "    \"\"\"\n",
    "    Inverse Document Frequency of a term.\n",
    "    \n",
    "    :param term_id : the term id \n",
    "    \"\"\"\n",
    "    df = term2numdocs[term_id]\n",
    "                       \n",
    "    return log(num_documents / df) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading google embeds\n",
    "\n",
    "with open(\"reduced_vectors_google.txt\",\"r\") as f:\n",
    "    g_embeds = {}\n",
    "    vec = []\n",
    "   \n",
    "    for i,l in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        line = l.split()\n",
    "        vec.extend(line)\n",
    "        if len(vec) == 301:\n",
    "            g_embeds[vec[0]] = [float(entry) for entry in vec[1:]]\n",
    "            vec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top 1000 docs for each query in the test set.\n",
    "tfidf_filter_test = tfidf_filter(n=1000, dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2doc_subset(subset, embeds, weight_model = \"tfidf\"):\n",
    "    '''\n",
    "    Creates dictionary of document embeddings, \"int_doc_id\" --> np_array of doc embed.\n",
    "    \n",
    "    :param subset: a subset of the data, for example top1000 by tfidf\n",
    "    :param embeds: either self trained by w2w or google embeds\n",
    "    :param weight_model : either \"tfidf\" or \"BM25\"\n",
    "    \"\"\"\n",
    "    '''\n",
    "    except_list = []\n",
    "    key_error = 0\n",
    "    doc_embeds = dict()\n",
    "    count = 0\n",
    "    count_words_below_5 = 0\n",
    "     # FOR ALL QUERIES\n",
    "    for query_id in subset.keys():  \n",
    "        # FOR ALL DOCUMENTS\n",
    "        for int_doc_id in subset[query_id]:  \n",
    "                \n",
    "            # discard all zero length docs\n",
    "            if document_lengths[int_doc_id] == 0:\n",
    "                    continue\n",
    "\n",
    "            _ , doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "            document_bow = collections.Counter(\n",
    "            token_id for token_id in doc_token_ids\n",
    "            if token_id > 0)\n",
    "        \n",
    "            document_vec = np.zeros(300)\n",
    "            normalization_const = 0\n",
    "            for token_id in list(document_bow.keys()):\n",
    "                # only if the term is in the w2v embeddings (i.e. it appears more than 5 times in collection)\n",
    "                try:\n",
    "                    # log normalized tf\n",
    "                    tf = 1 + log(document_bow[token_id])\n",
    "                    idf = IDF_all_terms(token_id)\n",
    "                    if weight_model == \"tfidf\":\n",
    "                        \n",
    "                        weight = tf * idf\n",
    "                        \n",
    "                    elif weight_model == \"BM25\":\n",
    "                        \n",
    "                        doc_len = sum(document_bow.values())\n",
    "                        \n",
    "                        weight = (2.2*tf)/(1.2*(0.25+0.75*(doc_len/avg_doc_length)) + tf) * idf\n",
    "                \n",
    "                    # weight * word2vec embed of term\n",
    "                   \n",
    "                    document_vec += weight * np.array(embeds[id2token[token_id]])\n",
    "                    \n",
    "                    normalization_const += 1\n",
    "                except KeyError:\n",
    "                    if token_id not in except_list:\n",
    "                        except_list.append(token_id)\n",
    "                    key_error +=1\n",
    "                    continue\n",
    "                \n",
    "            # only create document embedding if there was at least a term present in the w2v embeddings\n",
    "            if normalization_const != 0:\n",
    "                doc_embeds[int_doc_id] = document_vec / normalization_const\n",
    "                \n",
    "        return doc_embeds                \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word2query_subset(subset, embeds):\n",
    "    '''\n",
    "    Creates dictionary of query embeddings, \"query_id\" --> np_array of doc embed.\n",
    "\n",
    "    :param subset: a subset of the data, for example top1000 by tfidf\n",
    "    :param embeds: either self trained by w2w or google embeds\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    query_embeds = dict()\n",
    "    \n",
    "    for query_id in list(subset.keys()):  \n",
    "        query_term_embeds = np.zeros(300)\n",
    "        query_len = 0\n",
    "        for q_term in tokenized_queries[query_id]:\n",
    "            \n",
    "            try:\n",
    "                query_len += 1\n",
    "                query_term_embeds += np.array(embeds[id2token[q_term]])\n",
    "                \n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "        if query_len > 0:\n",
    "        \n",
    "            query_avg = query_term_embeds/query_len\n",
    "        \n",
    "        query_embeds[query_id] = query_avg\n",
    "       \n",
    "    return query_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('model_w2v.txt', binary=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc and query embeds of test set \n",
    "doc_embeds= word2doc_subset(tfidf_filter_test, model)\n",
    "doc_embeds_g = word2doc_subset(tfidf_filter_test,g_embeds)\n",
    "query_embeds = word2query_subset(tfidf_filter_test,model)\n",
    "query_embeds_g = word2query_subset(tfidf_filter_test,g_embeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(split):\n",
    "    if split != 'test' and split != 'validation':\n",
    "        raise ValueError(\"Only values for split: 'test', 'validation'.\")\n",
    "        \n",
    "    training_data = []\n",
    "    query_doc_pairs = []\n",
    "    \n",
    "    query_to_docs = test_ids if split == 'test' else validation_ids\n",
    "    \n",
    "    for query_id in query_to_docs.keys():\n",
    "        query_counter = collections.Counter(token_id \n",
    "                                            for token_id in token_ids\n",
    "                                            if token_id > 0\n",
    "                                            )\n",
    "        query_terms = list(query_counter.items())\n",
    "        query_tfidf = tfidf_model[query_terms]\n",
    "        query_vec_lsi = lsi[query_tfidf]\n",
    "        query_vec_lda = None\n",
    "\n",
    "        similarities = lsi_similarity_index[query_vec_lsi]\n",
    "\n",
    "        for int_doc_id, ext_doc_id in query_to_docstest_ids[query_id]: \n",
    "\n",
    "            # discard all zero length docs\n",
    "            if document_lengths[int_doc_id] == 0:\n",
    "                continue\n",
    "\n",
    "            scores = []\n",
    "\n",
    "            for score_fn in [tfidf, BM25, jelinek_mercer, dirichlet_prior, absolute_discounting, lsi, lda]:\n",
    "\n",
    "                cnt_qwords_in_doc = 0  # how many query words occ\n",
    "                score = 0\n",
    "\n",
    "                if score_fn == lsi:\n",
    "                    score = similarities[int_doc_id - 1]\n",
    "                if score_fn == lda:\n",
    "                    if query_vec_lda is None:\n",
    "                        query_vec_lda = lda[query_tfidf]\n",
    "                        query_distribution = [x[1] for x in query_vec_lda]\n",
    "\n",
    "                    doc_tfidf = tfidf_corpus[int_doc_id - 1]\n",
    "                    doc_distribution = [x[1] for x in lda[doc_tfidf]]\n",
    "                    score = - kl_divergence(query_distribution, doc_distribution)\n",
    "\n",
    "                for q_term in tokenized_queries[query_id]: \n",
    "\n",
    "                    try: \n",
    "                        document_term_freq = inverted_index[q_term][int_doc_id]\n",
    "                        cnt_qwords_in_doc += 1\n",
    "                    except KeyError:\n",
    "                        document_term_freq = 0  # query term is not in the document\n",
    "\n",
    "                    if score_fn == tfidf or score_fn == BM25:\n",
    "                        score += score_fn(int_doc_id, q_term, document_term_freq)\n",
    "                    elif score_fn == jelinek_mercer:\n",
    "                        score += log(score_fn(int_doc_id, q_term, document_term_freq, **{'lambda':0.1}))\n",
    "                    elif score_fn == dirichlet_prior:\n",
    "                        score += log(score_fn(int_doc_id, q_term, document_term_freq, mu=500))\n",
    "                    elif score_fn == absolute_discounting:\n",
    "                        score += log(score_fn(int_doc_id, q_term, document_term_freq, delta=0.9))\n",
    "\n",
    "                # transform back from log-probs to probs\n",
    "                if score_fn in [jelinek_mercer, dirichlet_prior, absolute_discounting]:\n",
    "                    score = exp(score)\n",
    "\n",
    "                # only score documents that contain at least one query term\n",
    "                if cnt_qwords_in_doc == 0:\n",
    "                    score = 0\n",
    "\n",
    "                scores.append(score)\n",
    "            training_data.append(scores)\n",
    "            query_doc_pairs.append((query_id, ext_doc_id))\n",
    "        \n",
    "        \n",
    "    y = np.zeros(len(training_data))\n",
    "\n",
    "    path = './ap_88_89/qrel_test' if split == 'test' else './ap_88_89/qrel_validation'\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            q_id = line[0]\n",
    "            doc_id = line[2]\n",
    "            label = line[3]\n",
    "\n",
    "            try:\n",
    "                idx = training_query_doc_pairs.index((q_id, doc_id))\n",
    "                y[idx] = int(label)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    return np.array(training_data), query_doc_pairs, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, X_doc_pairs, y = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logreg_crossval = LogisticRegressionCV(Cs=10,\n",
    "                                       fit_intercept=True,\n",
    "                                       cv=10, \n",
    "                                       dual=False,\n",
    "                                       penalty='l2', \n",
    "                                       scoring=None,\n",
    "                                       solver='lbfgs',\n",
    "                                       tol=0.0001,\n",
    "                                       max_iter=100, \n",
    "                                       class_weight=None,\n",
    "                                       intercept_scaling=1.0,\n",
    "                                       multi_class='ovr',\n",
    "#                                        multi_class='multinomial',\n",
    "                                       random_state=None)\n",
    "\n",
    "\n",
    "logreg = logreg_crossval.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = fit.predict_proba(X)\n",
    "logreg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 5: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
