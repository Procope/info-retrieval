{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Part\n",
    "\n",
    "##### Hypothesis Testing: the problem of multiple comparisons \n",
    "\n",
    "Given that:\n",
    "- $\\alpha$ is the Type I error for each test\n",
    "- $\\beta$ is the Type II error\n",
    "- $1 - \\beta$ is the power\n",
    "\n",
    "a) P($m^{th}$ experiment gives significant result | $m$ experiments lacking power to reject $H_0$) = $\\alpha (1 - \\beta)^{m-1}$,\n",
    "\n",
    "where $(1 - \\beta)^{m-1}$ is the probability that the first $m - 1$ experiments correctly conclude that the result is not significant, and $\\alpha$ is the probability that the next experiment will yield a Type I error. We can rephrase this event as\n",
    "> Experiment 1 correctly rejects $H_0$ **AND** Experiment 2 correctly rejects $H_0$ **AND ... AND** Experiment $m$ yields a Type I error.\n",
    "\n",
    "b) P(at least one significant result | $m$ experiments lacking power to reject $H_0$) = $m \\alpha$. \n",
    "\n",
    "We can rephrase this event as \n",
    "> Experiment 1 yields a Type I error **OR** Experiment 2 yields a Type I error **OR ... OR** Experiment $m$ yields a Type I error.\n",
    "\n",
    "which can be translated into an $m$-fold addition of the Type I error probability $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bias and unfairness in Interleaving experiments\n",
    "\n",
    "We present a situation where interleaving should produce a preference for one of the two ranking lists but \n",
    "it fails to do so and it assigns equal expected winning probability to both rankings.\n",
    "\n",
    "Let $d_3$ be the only relevant document, i.e. the only document the user will click on.\n",
    "In list A, $d_3$ is ranked as third, whereas $d_3$ is in the second position of Ranking A. Therefore, Ranking B is expected to win more often than Ranking B. Consider, however, the following rankings and the four lists that can be generated using Team Draft Interleaving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"interleaving_bias.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Simulate Rankings of Relevance for E and P\n",
    "\n",
    "import itertools\n",
    "\n",
    "relevance = [0, 1, 2]  # 0: N, 1: R, 2: HR\n",
    "combinations = list(itertools.product(relevance, repeat=5))\n",
    "ranking_pairs = list(itertools.product(combinations, repeat=2))\n",
    "\n",
    "# delete ratings that are the same for both  \n",
    "for i,k in enumerate(ranking_pairs):\n",
    "    if k[0] == k[1]:\n",
    "        del ranking_pairs[i]\n",
    "k = 10\n",
    "for pair in ranking_pairs[:k]:\n",
    "    print(pair)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_precision(query_judgement):\n",
    "    l = []\n",
    "    rel_docs = 0\n",
    "    \n",
    "    for i, judgement in enumerate(query_judgement, start=1):\n",
    "        if judgement != 0:\n",
    "            rel_docs += 1\n",
    "            l.append(rel_docs / i)\n",
    "            \n",
    "    if rel_docs == 0:\n",
    "        return 0\n",
    "    \n",
    "    return sum(l) / rel_docs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def dcg(query_judgement, k):\n",
    "    query_judgement = list(query_judgement)[:k]\n",
    "    dcg = 0\n",
    "    \n",
    "    for i, judgement in enumerate(query_judgement, start=1):\n",
    "        dcg += (2 ** judgement - 1) / log(i + 1, 2)\n",
    "        \n",
    "    return dcg\n",
    "\n",
    "def ndcg(query_judgement, k):\n",
    "    perfect_ordering = sorted(query_judgement, reverse=True)\n",
    "    perfect_score = dcg(perfect_ordering, k)\n",
    "    \n",
    "    if perfect_score == 0:\n",
    "        return 0\n",
    "    \n",
    "    return dcg(query_judgement, k) / perfect_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_rel = 2  # ad-hoc for 3 different judgements\n",
    "satisfaction_probs = [(2 ** rel - 1) / 2 ** max_rel for rel in range(3)]\n",
    "\n",
    "def err(query_judgement):\n",
    "    sat_probs = []\n",
    "    \n",
    "    for rel in query_judgement:\n",
    "        sat_probs.append(satisfaction_probs[rel])\n",
    "        \n",
    "    err = 0\n",
    "    \n",
    "    for i in range(1, len(query_judgement) + 1):\n",
    "        prod = sat_probs[i - 1]\n",
    "        \n",
    "        for j in range(1, i):\n",
    "            prod *= 1 - sat_probs[j]\n",
    "        \n",
    "        err += prod / i\n",
    "    \n",
    "#     if err == 0:\n",
    "#         return len(query_judgement)\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_rel = 2  # ad-hoc for 3 different judgements\n",
    "satisfaction_probs = [(2 ** rel - 1) / 2 ** max_rel for rel in range(3)]\n",
    "\n",
    "def err(query_judgement):\n",
    "    sat_probs = []\n",
    "    \n",
    "    for rel in query_judgement:\n",
    "        sat_probs.append(satisfaction_probs[rel])\n",
    "        \n",
    "    err = 0\n",
    "    prod = 1\n",
    "    \n",
    "    for i in range(1, len(query_judgement) + 1):\n",
    "        rel_prob = sat_probs[i - 1]\n",
    "        \n",
    "        err += prod * rel_prob / i\n",
    "        prod *= 1 - rel_prob\n",
    "            \n",
    "#     if err == 0:\n",
    "#         return len(query_judgement)\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/ err(ranking_pairs[11][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_pairs[242][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "def team_draft_interleaving(list_a, list_b):\n",
    "    '''\n",
    "    Implementation of the team draft interleaving algorithm\n",
    "    Args:\n",
    "        a, b: list\n",
    "    Return:\n",
    "        interleaved_list, chosen_list\n",
    "    '''\n",
    "    final_list = []\n",
    "    chosen = []  # list containing a/b values\n",
    "\n",
    "    # This is done so that, original lists are not modified\n",
    "    a = copy.deepcopy(list_a)\n",
    "    b = copy.deepcopy(list_b)\n",
    "    \n",
    "    while (len(a) > 0 and len(b) > 0):\n",
    "        choose_a = bool(random.getrandbits(1))\n",
    "\n",
    "        label_add = ['a', 'b']\n",
    "\n",
    "        current_a = a.pop(0)\n",
    "        current_b = b.pop(0)\n",
    "\n",
    "        to_add = list(set([current_a, current_b]))\n",
    "\n",
    "        if not choose_a:\n",
    "            to_add = to_add[::-1]  # reverse list\n",
    "            label_add = label_add[::-1]\n",
    "\n",
    "        # TODO: Check if t\n",
    "        if len(to_add) == 1:\n",
    "            del label_add[1]\n",
    "            \n",
    "        final_list += to_add\n",
    "        chosen += label_add\n",
    "\n",
    "        if current_b in a: a.remove(current_b)\n",
    "        if current_a in b: b.remove(current_a)\n",
    "\n",
    "    return final_list, chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_clicks(doc_size, num_clicks=2):\n",
    "    return random.sample(range(doc_size), num_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def team_draft_credit(pointer_list, simulated_clicks):\n",
    "    \n",
    "    credit_a, credit_b = 0, 0\n",
    "    \n",
    "    for idx in simulated_clicks:\n",
    "        credit_a += pointer_list[idx] == 'a'\n",
    "        credit_b += pointer_list[idx] == 'b'\n",
    "    \n",
    "    return credit_a, credit_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "b = [2, 3, 4, 1]\n",
    "\n",
    "interleaved, pointers = team_draft_interleaving(a, b)\n",
    "clicks = simulate_clicks(len(a), 2)\n",
    "\n",
    "print('-'*51)\n",
    "print('interleaved     :', interleaved)\n",
    "print('pointers     :', pointers)\n",
    "print('clicks       :', [interleaved[c] for c in clicks], ' {NOTE: these are docIDs}')\n",
    "print('credit (a, b):', team_draft_credit(pointers, clicks))\n",
    "print('-'*51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## click models\n",
    "\n",
    "# data:  SessionID, TimePassed, TypeOfAction, QueryID, RegionID, ListOfURLs\n",
    "\n",
    "data = []\n",
    "with open(\"YandexRelPredChallenge.txt\",\"r\") as yandex:\n",
    "    \n",
    "    for line in yandex:\n",
    "        data.append(line.strip().split('\\t'))\n",
    "        print(line.strip().split('\\t'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random click model: param and prediction\n",
    "\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rcm_param(data):\n",
    "    clicks = 0\n",
    "    number_docs_shown = 0\n",
    "    for line in data:\n",
    "        if line[2] == \"C\":\n",
    "            clicks += 1\n",
    "        \n",
    "        if line[2] == \"Q\":\n",
    "            number_docs_shown += len(line[5:])\n",
    "        \n",
    "    return clicks / number_docs_shown\n",
    "\n",
    "def rcm_predict(ranked_list, p):\n",
    "    clicks = []\n",
    "    for u in ranked_list:\n",
    "        click = np.random.binomial(1, p)\n",
    "        if click:\n",
    "            clicks.append(u)\n",
    "    return clicks\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rcm_predict(range(10), rcm_param(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simplified Dynamic Bayesian Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBN(ranked_list, \n",
    "        query, \n",
    "        attraction, \n",
    "        satisfaction, \n",
    "        examination, \n",
    "        continuation):\n",
    "    \n",
    "    q = query\n",
    "    clicks = []\n",
    "    \n",
    "    for r, u in enumerate(ranked_list, start=1):\n",
    "        click_prob = attraction[u][q] * examination[r]\n",
    "        \n",
    "        if np.random.binomial(1, click_prob):\n",
    "            clicks.append(u)\n",
    "            \n",
    "            if np.random.binomial(1, satisfaction[u][q]):\n",
    "                break\n",
    "            if np.random.binomial(1, 1 - continuation):\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, index):\n",
    "        self.id = index\n",
    "        self.query_to_docs = OrderedDict()\n",
    "        self.query_to_click_rank = OrderedDict()\n",
    "        \n",
    "        \n",
    "    def add_query(self, query, doc_list):\n",
    "        \n",
    "        if query in self.query_to_docs.keys():\n",
    "            \n",
    "            if self.query_to_docs[query] == doc_list:\n",
    "                self.query_to_docs.move_to_end(query)\n",
    "            else:\n",
    "                del self.query_to_docs[query]\n",
    "                self.query_to_docs[query] = doc_list\n",
    "                \n",
    "        else:\n",
    "            self.query_to_docs[query] = doc_list\n",
    "        \n",
    "        \n",
    "    def add_click(self, query, click, rank):\n",
    "        try:\n",
    "            self.query_to_click_rank[query].append((click, rank))\n",
    "        except KeyError:\n",
    "            self.query_to_click_rank[query] = [(click, rank)]\n",
    "    \n",
    "    \n",
    "    def docs_till_last_clicked(self):\n",
    "        query_to_first_l_docs = OrderedDict()\n",
    "        \n",
    "        try:\n",
    "            last_doc, rank = list(self.query_to_click_rank.values())[0][-1]\n",
    "        except IndexError:\n",
    "            rank = len(list(self.query_to_docs.values())[0])\n",
    "        \n",
    "        for query, doc_list in self.query_to_docs.items():\n",
    "            query_to_first_l_docs[query] = doc_list[:rank]\n",
    "        \n",
    "        return query_to_first_l_docs\n",
    "    \n",
    "    \n",
    "    def last_clicked(self):\n",
    "        try:\n",
    "            doc, rank = list(self.query_to_click_rank.values())[0][-1]\n",
    "            return doc\n",
    "        except IndexError:\n",
    "            return -1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sessions(filepath):\n",
    "    sessions = []\n",
    "    \n",
    "    with open(filepath, 'r') as log:\n",
    "        sess = Session(0)\n",
    "        for line in log:\n",
    "            line = line.strip().split('\\t')\n",
    "            \n",
    "            if int(line[0]) != sess.id:\n",
    "                sessions.append(copy.deepcopy(sess))\n",
    "                sess = Session(int(line[0]))\n",
    "            \n",
    "            if line[2] == 'Q':\n",
    "                sess.add_query(int(line[3]), list(map(int, line[5:])))\n",
    "                \n",
    "            if line[2] == 'C':\n",
    "                last_query = list(sess.query_to_docs.keys())[-1]\n",
    "                try:\n",
    "                    rank = sess.query_to_docs[last_query].index(int(line[3])) + 1\n",
    "                    sess.add_click(last_query, int(line[3]), rank)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "#                     print(line[3], sess.query_to_docs[last_query])\n",
    "    \n",
    "    return sessions\n",
    "\n",
    "\n",
    "def estimate_SDBN_params(sessions):\n",
    "    S = defaultdict(int)\n",
    "    S_prime = defaultdict(int)\n",
    "    alpha = defaultdict(int)\n",
    "    sigma = defaultdict(int)\n",
    "    epsilon = [1]\n",
    "    \n",
    "    for sess in sessions:\n",
    "        for query, docs in sess.docs_till_last_clicked().items():\n",
    "            for doc in docs:\n",
    "                S[(doc, query)] += 1\n",
    "        \n",
    "        for query in sess.query_to_click_rank.keys():\n",
    "            click_rank_pairs = sess.query_to_click_rank[query]\n",
    "            \n",
    "            for doc, rank in click_rank_pairs:\n",
    "                S_prime[(doc, query)] += 1\n",
    "                alpha[(doc, query)] += 1\n",
    "                \n",
    "                if sess.last_clicked() == doc:\n",
    "                    sigma[(doc, query)] += 1\n",
    "        \n",
    "        for doc, query in alpha.keys():\n",
    "            try:\n",
    "                alpha[(doc, query)] /= S[(doc, query)]\n",
    "            except ZeroDivisionError:\n",
    "                alpha[(doc, query)] = 0\n",
    "            try:  \n",
    "                sigma[(doc, query)] /= S_prime[(doc, query)]\n",
    "            except ZeroDivisionError:\n",
    "                sigma[(doc, query)] = 0\n",
    "            \n",
    "    return alpha, sigma, epsilon\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yandex_sessions = read_sessions(\"YandexRelPredChallenge.txt\")\n",
    "alpha, sigma, epsilon = estimate_SDBN_params(yandex_sessions)\n",
    "\n",
    "print(alpha, sigma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
